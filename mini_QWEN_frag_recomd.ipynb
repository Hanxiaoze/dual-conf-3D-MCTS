{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use GPU\n"
     ]
    }
   ],
   "source": [
    "# part 1: 导入相关的 package\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import BRICS\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import math\n",
    "import ast\n",
    "\n",
    "seed = 888\n",
    "\n",
    "def set_random_seed(seed=88):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_random_seed(seed)\n",
    "\n",
    "# 获取当前时间\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d__%H-%M-%S\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('use GPU')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('use CPU')\n",
    "    device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 64   # 这里其实应该是文本的最大长度（ max_seq_len）\n",
    "    seq_len = block_size\n",
    "    batch_size: int = 512\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 16\n",
    "    num_heads = n_head\n",
    "    n_embd: int = 256  # n_embd 也叫 hidden_dim, hiden_size, 这里我同时设置了和 embed_dim 一样\n",
    "    d_model = n_embd    \n",
    "    v_head_dim = 32\n",
    "    kv_lora_rank = 16\n",
    "    q_lora_rank = 3 * kv_lora_rank\n",
    "    rope_head_dim = 32\n",
    "    nope_head_dim = 16\n",
    "    dropout: float = 0.1\n",
    "    # # tiktoken 使用的是 GPT-2 的词表，大约有 50257 个token\n",
    "    vocab_size: int = 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CO', 'CCC', 'CC=O', 'O=CO', 'CN', 'Oc1ccccc1', 'CNC', 'CCC=O', 'CC(=O)O', 'CCO', 'CCCC', 'CCC(=O)O', 'Cc1ccccc1', 'CC(C)O', 'CC(N)C(=O)O', 'CCCCC', 'CN1CCNCC1', 'O=CCO', 'CCCC=O', 'CCN', 'NS(=O)(=O)c1ccccc1', 'Nc1ccccc1', 'CS', 'CCCO', 'Cc1cccc(C)c1', 'CCCC(=O)O', 'CCCCC(=O)O', 'CCCCCC', 'O=C(O)CCCC(=O)O', 'CC(O)CO', 'CN1CCCCC1', 'CC(N)C=O', 'O=CNO', 'Cc1c[nH]c(=O)[nH]c1=O', 'CC(F)(F)F', 'NCC=O', 'CC(C)CCC=O', 'Oc1cccc(O)c1', 'Nc1ccc([SH](=O)=O)cc1', 'CC(C)C=O', 'CCCCC=O', 'O=CCCC(=O)O', 'Cn1cccn1', 'CCC(C)C', 'NC(CCC=O)C(=O)O']\n",
      "45\n",
      "['N', 'c1ccccc1', 'O', 'C', 'CC', 'C=O', 'c1ccncc1', 'Clc1ccccc1', 'C1CCNCC1', 'Fc1ccccc1', 'O=P(O)(O)O', 'FC(F)F', 'CC(C)C', 'NC=O', 'C1CCNC1', 'OC1COCC1O', 'C1CNCCN1', 'C1CC1', 'S', 'C1CCCCC1', 'c1cncnc1', 'C1COCCN1', 'OC1COCC(O)C1O', 'c1cn[nH]c1', 'c1c[nH]cn1', 'c1ccc2[nH]ccc2c1', 'c1ccsc1', 'c1ccc2ccccc2c1', 'O=[SH](=O)c1ccccc1', 'Nc1ncnc2[nH]cnc12', 'Clc1cccc(Cl)c1', 'O=[N+]([O-])c1ccccc1', 'OC1CCOC1', 'C1CCCC1', 'O=P(O[3*])(O)OP(=O)(O)O[3*]', 'Fc1cccc(F)c1', 'c1cscn1', 'c1ccc2[nH]cnc2c1', 'Oc1ccccc1O', 'c1ccc2ncccc2c1', 'O=c1cc[nH]c(=O)[nH]1', 'C[SH](=O)=O', 'Brc1ccccc1', 'N#Cc1ccccc1', 'O=C1CCCN1', 'C=CC', 'c1ccoc1', 'N=C(N)c1ccccc1', 'c1ncc2nc[nH]c2n1', 'C[NH+](C)C', 'c1ccc2ncncc2c1', 'N=C(N)N', 'Nc1cc[nH]c(=O)n1', 'C1CCOC1', 'Clc1ccccc1Cl', 'c1ccc2c(c1)OCO2', 'CC1(C)CN2C(=O)CC2S1', 'c1ccc2[nH]ncc2c1', 'CC1=C(C(=O)O)N2C(=O)CC2SC1', 'C1CCOCC1', 'c1nc[nH]n1', 'NS(=O)(=O)O', 'OC1CCOCC1O', 'c1ccc2c(c1)Nc1ccccc1S2', 'xxxx', 'EOS']\n",
      "66\n",
      "{'CO': ['C', 'O'], 'CCC': ['C', 'CC'], 'CC=O': ['C', 'C=O'], 'O=CO': ['O', 'C=O'], 'CN': ['C', 'N'], 'Oc1ccccc1': ['O', 'c1ccccc1'], 'CNC': ['C', 'C', 'N'], 'CCC=O': ['CC', 'C=O'], 'CC(=O)O': ['C', 'C=O', 'O'], 'CCO': ['CC', 'O'], 'CCCC': ['CC', 'CC'], 'CCC(=O)O': ['CC', 'C=O', 'O'], 'Cc1ccccc1': ['C', 'c1ccccc1'], 'CC(C)O': ['CC', 'C', 'O'], 'CC(N)C(=O)O': ['CC', 'N', 'C=O', 'O'], 'CCCCC': ['CC', 'CC', 'C'], 'CN1CCNCC1': ['C', 'C1CNCCN1'], 'O=CCO': ['C=O', 'C', 'O'], 'CCCC=O': ['C', 'CC', 'C=O'], 'CCN': ['CC', 'N'], 'NS(=O)(=O)c1ccccc1': ['c1ccccc1', 'NS(=O)(=O)O'], 'Nc1ccccc1': ['N', 'c1ccccc1'], 'CS': ['C', 'S'], 'CCCO': ['C', 'CC', 'O'], 'Cc1cccc(C)c1': ['C', 'C', 'c1ccccc1'], 'CCCC(=O)O': ['C', 'CC', 'C=O', 'O'], 'CCCCC(=O)O': ['CC', 'CC', 'C=O', 'O'], 'CCCCCC': ['CC', 'CC', 'CC'], 'O=C(O)CCCC(=O)O': ['C=O', 'O', 'C', 'CC', 'C=O', 'O'], 'CC(O)CO': ['CC', 'O', 'C=O'], 'CN1CCCCC1': ['C', 'C1CCNCC1'], 'CC(N)C=O': ['CC', 'N', 'C=O'], 'O=CNO': ['C=O', 'N', 'O'], 'Cc1c[nH]c(=O)[nH]c1=O': ['O=c1cc[nH]c(=O)[nH]1', 'C'], 'CC(F)(F)F': ['FC(F)F', 'C'], 'NCC=O': ['N', 'C', 'C=O'], 'CC(C)CCC=O': ['CC', 'C', 'CC', 'C=O'], 'Oc1cccc(O)c1': ['O', 'O', 'c1ccccc1'], 'Nc1ccc([SH](=O)=O)cc1': ['O=[SH](=O)c1ccccc1', 'N'], 'CC(C)C=O': ['CC', 'C', 'C=O'], 'CCCCC=O': ['CC', 'CC', 'C=O'], 'O=CCCC(=O)O': ['C=O', 'CC', 'C=O', 'O'], 'Cn1cccn1': ['C', 'c1cn[nH]c1'], 'CCC(C)C': ['CC', 'CC', 'C'], 'NC(CCC=O)C(=O)O': ['N', 'C', 'CC', 'C=O', 'C=O', 'O']}\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "datasmi_mol_elem = []\n",
    "datasmi_mol_2nd = []\n",
    "datasmi_mol_total = []\n",
    "data_mol_elem = []\n",
    "data_mol_2nd = []\n",
    "data_mol_total = []\n",
    "data_1_path = './mol_in_chem_space.txt'\n",
    "data_2_path = './mol_not_in_chem_space.txt'\n",
    "\n",
    "\n",
    "elem_frag_smi_list = []\n",
    "frag_smi_to_2nd_break_list = []\n",
    "frag_smi_2nd_break_dict = {}\n",
    "\n",
    "\n",
    "with open(\"./my_frags_3.txt\", \"r\") as f:\n",
    "    frag_smi_to_2nd_break_list = [line.split(':')[0].strip() for line in f.readlines()[:109] if ':' in line]\n",
    "\n",
    "with open(\"./my_frags_3.txt\", \"r\") as f:\n",
    "    elem_frag_smi_list = [line.strip() for line in f.readlines()[:109] if ':' not in line]\n",
    "    elem_frag_smi_list.append('xxxx')   # 65 个片段库  + EOS\n",
    "    elem_frag_smi_list.append('EOS')\n",
    "\n",
    "print(frag_smi_to_2nd_break_list)\n",
    "print(len(frag_smi_to_2nd_break_list))\n",
    "print(elem_frag_smi_list)\n",
    "print(len(elem_frag_smi_list))\n",
    "\n",
    "def read_txt_to_2nd_break_dict(txt_file):\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        for line in f.readlines()[:109]:\n",
    "            if ':' in line:\n",
    "                frag_smi_2nd_break_dict[line.split(':')[0].strip()] = [ s.strip() for s in line.split(':')[-1].strip().split(',')]\n",
    "    return frag_smi_2nd_break_dict\n",
    "\n",
    "frag_smi_2nd_break_dict = read_txt_to_2nd_break_dict(\"./my_frags_3.txt\")\n",
    "print(frag_smi_2nd_break_dict)\n",
    "print(len(frag_smi_2nd_break_dict))\n",
    "\n",
    "# 创建分子片段到索引的映射\n",
    "fragment_to_idx = {fragment: idx for idx, fragment in enumerate(elem_frag_smi_list)}\n",
    "idx_to_fragment = {idx: fragment for fragment, idx in fragment_to_idx.items()}\n",
    "idx_to_fragment.update({66: 'EOS'})\n",
    "\n",
    "with open('./mol_in_chem_space.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        datasmi_mol_elem.append(ast.literal_eval(line.split(':')[2].strip()))\n",
    "        \n",
    "\n",
    "with open('./mol_not_in_chem_space.txt', 'r') as f:\n",
    "    for line in f.readlines():        \n",
    "        # 将字符串按 ':' 分割为列表\n",
    "        parts = line.split(':')\n",
    "        # 去掉前两个字段，保留后面的部分，并重新组合为一个字符串\n",
    "        result = ':'.join(parts[2:])        \n",
    "        # word_freq_vector = word_freq_func(ast.literal_eval(result))\n",
    "        # data_mol_2nd.append(word_freq_vector)\n",
    "        list = []\n",
    "        for i in result.strip()[1:-1].split(','):\n",
    "            list.append(i.strip()[1:-1])\n",
    "        datasmi_mol_2nd.append(list)\n",
    "\n",
    "datasmi_mol_total = datasmi_mol_elem + datasmi_mol_2nd  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63714\n"
     ]
    }
   ],
   "source": [
    "# 自定义 Dataset 类，支持多个分子片段序列\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, molecule_sequences, fragment_to_idx):\n",
    "        \"\"\"\n",
    "        molecule_sequences: 分子片段序列的列表，每个元素是一个由多个分子片段组成的序列\n",
    "        fragment_to_idx: 分子片段到索引的映射\n",
    "        \"\"\"\n",
    "        self.molecule_sequences = molecule_sequences\n",
    "        self.fragment_to_idx = fragment_to_idx\n",
    "        self.block_size = GPTConfig().block_size\n",
    "        self.encoded_sequences = []\n",
    "\n",
    "        # 将分子片段序列转化为对应的索引序列\n",
    "        for sequence in self.molecule_sequences:\n",
    "            if len(sequence) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                encoded_sequence = []\n",
    "                for frag in sequence:\n",
    "                    if 'xxxx' in frag:\n",
    "                        encoded_sequence.append(self.fragment_to_idx['xxxx'])\n",
    "                    else:\n",
    "                        encoded_sequence.append(self.fragment_to_idx[frag])\n",
    "                encoded_sequence.append(66)  # EOS = 66\n",
    "                self.encoded_sequences.extend(encoded_sequence)\n",
    "\n",
    "        # 将超长文本分割成训练样本\n",
    "        self.encoded_data = []\n",
    "        for i in range(0, len(self.encoded_sequences)):\n",
    "            chunk = self.encoded_sequences[i:i+self.block_size+1]\n",
    "            # 如果长度不够，用 eos_token 填充\n",
    "            if len(chunk) < self.block_size + 1:\n",
    "                chunk = chunk + [66] * (self.block_size + 1 - len(chunk))  # EOS = 66\n",
    "            self.encoded_data.append(chunk)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.encoded_data[idx]\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "        \n",
    "\n",
    "\n",
    "# 创建 Dataset 和 DataLoader 实例\n",
    "dataset = MoleculeDataset(datasmi_mol_total, fragment_to_idx)\n",
    "print(len(dataset))\n",
    "\n",
    "# split traindataset to train and val\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [0.9, 0.1])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=GPTConfig().batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=GPTConfig().batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS norm, ROPE, MLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ohara.embedings_pos.rotatry import precompute_freqs_cis\n",
    "from ohara.embedings_pos.rotatry import apply_rope\n",
    "from ohara.modules.norm import RMSNorm\n",
    "\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi Head Latent Attention \n",
    "    paper: https://arxiv.org/pdf/2405.04434\n",
    "    \n",
    "    TLDR: \n",
    "    kv are low ranks, this verient of attention project q,k,v to low rank to save memory,\n",
    "    replace linear with lora(ish) layers\n",
    "\n",
    "    source: https://github.com/joey00072/Multi-Head-Latent-Attention-MLA-\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert config.v_head_dim is not None , f\"v_head_dim is not defined {config.v_head_dim=}\"\n",
    "        assert config.q_lora_rank is not None , f\"q_lora_rank is not defined {config.q_lora_rank=}\"\n",
    "        assert config.kv_lora_rank is not None , f\"kv_lora_rank is not defined {config.kv_lora_rank=}\"\n",
    "        assert config.rope_head_dim is not None , f\"rope_head_dim is not defined {config.rope_head_dim=}\"\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        self.dim = config.d_model\n",
    "        self.num_heads = config.num_heads\n",
    "        self.v_head_dim = config.v_head_dim\n",
    "        \n",
    "        self.nope_head_dim = config.nope_head_dim\n",
    "        self.rope_head_dim = config.rope_head_dim\n",
    "        \n",
    "        self.q_lora_rank = config.q_lora_rank\n",
    "        self.kv_lora_rank = config.kv_lora_rank\n",
    "        \n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        # note: head dim of query and key if different from head dim of value\n",
    "        \n",
    "        # (attention_dim == num_head*head_dim) > d_model in deepseekv2\n",
    "        # this is dim between wV and wQ\n",
    "        self.value_dim = self.num_heads * self.v_head_dim\n",
    "        \n",
    "        # this is dims between wQ and wK\n",
    "        self.nope_dim = self.num_heads * self.nope_head_dim\n",
    "        self.rope_dim = self.num_heads * self.rope_head_dim  \n",
    "        \n",
    "        # query compression\n",
    "        self.compress_q_linear = nn.Linear(self.dim, self.q_lora_rank, bias=False)  # W_DQ\n",
    "        self.decompress_q_nope = nn.Linear(self.q_lora_rank, self.nope_dim, bias=False)\n",
    "        self.decompress_q_rope = nn.Linear(self.q_lora_rank, self.rope_dim, bias=False)\n",
    "        self.q_norm = RMSNorm(dim=self.q_lora_rank)\n",
    "        \n",
    "        \n",
    "        # key and value compression\n",
    "        self.compress_kv_linear = nn.Linear(self.dim, self.kv_lora_rank, bias=False)  # W_DKV\n",
    "        self.decompress_k_nope = nn.Linear(self.kv_lora_rank, self.nope_dim, bias=False)\n",
    "        self.decompress_v_linear = nn.Linear(self.kv_lora_rank, self.value_dim, bias=False)\n",
    "        self.kv_norm = RMSNorm(dim=self.kv_lora_rank)\n",
    "        \n",
    "        \n",
    "        self.k_rope_linear = nn.Linear(self.dim, self.rope_head_dim  , bias=False)\n",
    "        # self.rope_norm = RMSNorm(self.rope_dim) # not in deepseekv2\n",
    "\n",
    "        self.proj = nn.Linear(self.value_dim , self.dim, bias=False)\n",
    "        self.res_dropout = nn.Dropout(p=config.dropout)\n",
    "        self.freqs_cis = precompute_freqs_cis(config.rope_head_dim, config.seq_len)\n",
    "        self.freqs_cis = (self.freqs_cis[0].to(device), self.freqs_cis[0].to(device))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x: Tensor):        \n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        # print(f'batch_size: {batch_size}')\n",
    "\n",
    "        # 随机生成一个mask用于遮掩部分位置，避免计算某些位置的注意力\n",
    "        mask = torch.tril(torch.ones(batch_size, seq_len, seq_len))  # 上三角为0，下三角为1，用于 causal mask\n",
    "\n",
    "\n",
    "        compressed_q = self.compress_q_linear(x)\n",
    "        norm_q = self.q_norm(compressed_q)\n",
    "        query_nope: Tensor = self.decompress_q_nope(norm_q)\n",
    "        query_rope: Tensor = self.decompress_q_rope(norm_q)\n",
    "\n",
    "        compressed_kv = self.compress_kv_linear(x)\n",
    "        norm_kv = self.kv_norm(compressed_kv)\n",
    "        key_nope: Tensor = self.decompress_k_nope(norm_kv)\n",
    "        value: Tensor = self.decompress_v_linear(norm_kv)\n",
    "\n",
    "        key_rope: Tensor = self.k_rope_linear(x)\n",
    "\n",
    "        query_nope = query_nope.view(batch_size, seq_len, self.num_heads, self.nope_head_dim).transpose(1, 2)\n",
    "        query_rope = query_rope.view(batch_size, seq_len, self.num_heads, self.rope_head_dim).transpose(1, 2)\n",
    "\n",
    "        key_rope = key_rope.view(batch_size, seq_len, 1, self.rope_head_dim).transpose(1, 2)\n",
    "        key_nope = key_nope.view(batch_size, seq_len, self.num_heads, self.nope_head_dim).transpose(1, 2)\n",
    "\n",
    "        value = value.view(batch_size, seq_len, self.num_heads, self.v_head_dim).transpose(1, 2)\n",
    "\n",
    "        # *** the line that fixes MLA :) ***\n",
    "        key_rope = key_rope / self.num_heads\n",
    "\n",
    "        q_rope, k_rope = apply_rope(query_rope, key_rope, cis=self.freqs_cis)\n",
    "\n",
    "        q_recombined = torch.empty((batch_size, self.num_heads, seq_len, self.rope_head_dim + self.nope_head_dim), device=x.device)\n",
    "        k_recombined = torch.empty((batch_size, self.num_heads, seq_len, self.rope_head_dim + self.nope_head_dim), device=x.device)\n",
    "\n",
    "        q_recombined[:, :, :, :self.nope_head_dim] = query_nope\n",
    "        q_recombined[:, :, :, self.nope_head_dim:] = q_rope\n",
    "\n",
    "        k_recombined[:, :, :, :self.nope_head_dim] = key_nope\n",
    "        k_recombined[:, :, :, self.nope_head_dim:] = k_rope\n",
    "\n",
    "        # Apply the mask here. Mask should be of shape (batch_size, 1, seq_len, seq_len) to match the attention logits\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # Add the head dimension to the mask (batch_size, 1, seq_len, seq_len)\n",
    "            # The mask should be applied to the attention logits (query_key matrix), with a large negative value (e.g., -1e9)\n",
    "            # to prevent attention to those positions\n",
    "            mask = mask.expand(-1, self.num_heads, -1, -1)\n",
    "            mask = mask.to(x.device)\n",
    "            # print(f'q_recombined: {q_recombined.shape}')\n",
    "            # print(f'k_recombined: {k_recombined.shape}')\n",
    "            # print(f'value: {value.shape}')\n",
    "            # print(f'mask: {mask.shape}')\n",
    "            output = F.scaled_dot_product_attention(\n",
    "                q_recombined, k_recombined, value, attn_mask=mask, is_causal=True, dropout_p=self.dropout\n",
    "            )\n",
    "        else:\n",
    "            # If no mask is provided, just perform the attention as usual\n",
    "            output = F.scaled_dot_product_attention(\n",
    "                q_recombined, k_recombined, value, is_causal=True, dropout_p=self.dropout\n",
    "            )\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.num_heads * self.v_head_dim)\n",
    "\n",
    "        output = self.proj(output)\n",
    "        output = self.res_dropout(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    # 单头注意力机制\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.head_size = config.head_size\n",
    "        self.key = nn.Linear(config.n_embd, config.head_size)\n",
    "        self.value = nn.Linear(config.n_embd, config.head_size)\n",
    "        self.query = nn.Linear(config.n_embd, config.head_size)\n",
    "\n",
    "        # 尝试学习新的写法，attention_mask 通过 register_buffer 注册\n",
    "        # 因为不用计算 梯度，所以节约内存和显存，速度也更快\n",
    "        self.register_buffer(\n",
    "            'attention_mask', \n",
    "            torch.tril(\n",
    "                torch.ones(config.block_size, config.block_size)\n",
    "            ))\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, hidden_size = x.size()\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        q = self.query(x)\n",
    "        weight = q @ k.transpose(-2, -1)   # @ 就是 torch.matmul 的简化写法\n",
    "        weight = weight.masked_fill(\n",
    "            self.attention_mask[:seq_len, :seq_len] == 0, \n",
    "            float('-inf')\n",
    "        )\n",
    "        weight = F.softmax(weight, dim=-1) / math.sqrt(self.head_size)\n",
    "        weight = self.dropout(weight)\n",
    "        out = weight @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                SingleHeadAttention(config)\n",
    "                for _ in range(config.n_head)\n",
    "            ]\n",
    "        )\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.cat(\n",
    "            [h(x) for h in self.heads], \n",
    "            dim=-1\n",
    "        )\n",
    "        output = self.proj(output)\n",
    "        output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    # 实际上就是 MLP\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# 接下来就是一个完整的 Block\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        head_size = config.n_embd // config.n_head\n",
    "        # self.att = MultiHeadAttention(config)\n",
    "        self.att = MultiHeadLatentAttention(config)\n",
    "        self.ffn = FeedForward(config)\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.att(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# 使用 TensorBoard 记录训练信息\n",
    "writer = SummaryWriter(f'tensorboard_run_nn_models/frag_recomd_mini_QWEN_{current_time}')  # 创建 SummaryWriter 实例\n",
    "\n",
    "\n",
    "# 以后会讲  MLA ,  MOE, DPO 完全手写\n",
    "# 完整的  GPT model\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # TODO position embedding --> ROPE\n",
    "        # TODO layer norm --> RMS norm\n",
    "        # TODO MLP --> swiglu\n",
    "        # TODO MHA多头注意力 --> GPA\n",
    "        self.block_size = GPTConfig().batch_size\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(config) for _ in range(config.n_layer)]\n",
    "        )\n",
    "        self.ln_final = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # linear (4 -> 8)； weight shape 是记上是 8 * 4，\n",
    "        # 所以 embedding weight 和 lm_head weight 是共享的\n",
    "        # 这里学习一下 tie weight。\n",
    "        # 这是为了减少参数，加快训练；（现在 25的 SLM 很多都这样做了，注意⚠️）\n",
    "        self.token_embedding_table.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # 这里使用的是正态分布初始化\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx 是输入的 token ids\n",
    "        batch, seq_len = idx.size()\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "\n",
    "        # seq 长度是这次输入的最大长度\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            # 要确保 位置编码和输入的 idx 在同一个设备上\n",
    "            torch.arange(seq_len, device=idx.device)\n",
    "        )\n",
    "        # 有一个经典题目：为什么 embedding 和 position 可以相加？\n",
    "        x = token_emb + pos_emb   # shape is (batch, seq_len, n_embd)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_final(x)\n",
    "        logits = self.lm_head(x)   # shape is (batch, seq_len, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch, seq_len, vocab_size = logits.size()\n",
    "            logits = logits.view(batch * seq_len, vocab_size)\n",
    "            targets = targets.view(batch * seq_len)\n",
    "            loss = F.cross_entropy(logits, targets)  # 这里是每个时间 T 步的平均loss, 不受batch大小影响\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 如果序列太长，只取最后 block_size 个token\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            # 获取预测\n",
    "            logits, _ = self(idx_cond)\n",
    "            # 只关注最后一个时间步的预测\n",
    "            logits = logits[:, -1, :]  # becomes (B, vocab_size)\n",
    "            # 应用softmax获取概率\n",
    "            probs = F.softmax(logits, dim=-1)   # (B, vocab_size)\n",
    "            # 采样下一个token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # 附加到序列上\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(GPTConfig())\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 4.422784 M\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params / 1e6} M\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "# 设置 cosine 学习率\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 4.277499\n",
      "Epoch: 0, Batch: 11, Loss: 2.549685\n",
      "Epoch: 0, Batch: 22, Loss: 2.406250\n",
      "Epoch: 0, Batch: 33, Loss: 2.355066\n",
      "Epoch: 0, Batch: 44, Loss: 2.351576\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val_loss\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m---> 41\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Loss/train\u001b[39m\u001b[38;5;124m'\u001b[39m, train_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader), epoch)\n\u001b[1;32m     43\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(model, val_loader, device)\n",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, train_loader, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 反向传播\u001b[39;00m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/admet_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/admet_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# 有一个经典题目：为什么 embedding 和 position 可以相加？\u001b[39;00m\n\u001b[1;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m token_emb \u001b[38;5;241m+\u001b[39m pos_emb   \u001b[38;5;66;03m# shape is (batch, seq_len, n_embd)\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_final(x)\n\u001b[1;32m    143\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)   \u001b[38;5;66;03m# shape is (batch, seq_len, vocab_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/admet_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/admet_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/admet_ai/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/admet_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/admet_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     85\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x))\n\u001b[0;32m---> 86\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/admet_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/admet_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/admet_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/admet_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/admet_ai/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/admet_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/admet_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/admet_ai/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练循环\n",
    "def train(model, optimizer, scheduler, train_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        # 将数据移到设备上\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        logits, loss = model(x, targets=y)\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 调整学习率\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        if batch_idx % (len(train_loader)//10) == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.6f}')\n",
    "    \n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def eval(model, val_loader, device):\n",
    "    # 验证\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, loss = model(x, targets=y)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "for epoch in range(1000):\n",
    "    train_loss = train(model, optimizer, scheduler, train_loader, device)\n",
    "    writer.add_scalar('Average Loss/train', train_loss/len(train_loader), epoch)\n",
    "    val_loss = eval(model, val_loader, device)\n",
    "    writer.add_scalar('Average Loss/validate', val_loss/len(val_loader), epoch)\n",
    "    print(f'Epoch: {epoch}, Average Train Loss: {train_loss/len(train_loader):.6f}, Average Val Loss: {val_loss/len(val_loader):.6f}')\n",
    "\n",
    "    # 保存模型\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    if epoch+1 >= 50 and (epoch + 1) % 20 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': avg_val_loss,\n",
    "        }\n",
    "        # 保存每个epoch的模型\n",
    "        os.makedirs(f'./model_save_pth/mini_QWEN_frag_recomd_{current_time}', exist_ok=True)\n",
    "        torch.save(checkpoint, f'./model_save_pth/mini_QWEN_frag_recomd_{current_time}/model_epoch_{epoch+1}.pt')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重载 .pth 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape: torch.Size([1, 2])\n",
      "y: tensor([[ 1, 64]], device='cuda:0')\n",
      "['N', 'c1ccccc1', 'O', 'C', 'CC', 'C=O', 'c1ccncc1', 'Clc1ccccc1', 'C1CCNCC1', 'Fc1ccccc1', 'O=P(O)(O)O', 'FC(F)F', 'CC(C)C', 'NC=O', 'C1CCNC1', 'OC1COCC1O', 'C1CNCCN1', 'C1CC1', 'S', 'C1CCCCC1', 'c1cncnc1', 'C1COCCN1', 'OC1COCC(O)C1O', 'c1cn[nH]c1', 'c1c[nH]cn1', 'c1ccc2[nH]ccc2c1', 'c1ccsc1', 'c1ccc2ccccc2c1', 'O=[SH](=O)c1ccccc1', 'Nc1ncnc2[nH]cnc12', 'Clc1cccc(Cl)c1', 'O=[N+]([O-])c1ccccc1', 'OC1CCOC1', 'C1CCCC1', 'O=P(O[3*])(O)OP(=O)(O)O[3*]', 'Fc1cccc(F)c1', 'c1cscn1', 'c1ccc2[nH]cnc2c1', 'Oc1ccccc1O', 'c1ccc2ncccc2c1', 'O=c1cc[nH]c(=O)[nH]1', 'C[SH](=O)=O', 'Brc1ccccc1', 'N#Cc1ccccc1', 'O=C1CCCN1', 'C=CC', 'c1ccoc1', 'N=C(N)c1ccccc1', 'c1ncc2nc[nH]c2n1', 'C[NH+](C)C', 'c1ccc2ncncc2c1', 'N=C(N)N', 'Nc1cc[nH]c(=O)n1', 'C1CCOC1', 'Clc1ccccc1Cl', 'c1ccc2c(c1)OCO2', 'CC1(C)CN2C(=O)CC2S1', 'c1ccc2[nH]ncc2c1', 'CC1=C(C(=O)O)N2C(=O)CC2SC1', 'C1CCOCC1', 'c1nc[nH]n1', 'NS(=O)(=O)O', 'OC1CCOCC1O', 'c1ccc2c(c1)Nc1ccccc1S2', 'xxxx', 'EOS']\n",
      "y_frag_list: ['c1ccccc1', 'xxxx']\n",
      "probs.shape: torch.Size([1, 67])\n",
      "probs.shape: torch.Size([67])\n",
      "probs: tensor([7.4723e-02, 4.3973e-02, 1.4070e-01, 1.9393e-01, 7.6124e-02, 4.4521e-02,\n",
      "        3.7474e-03, 5.3233e-03, 9.0064e-03, 4.7765e-03, 3.5975e-03, 1.1414e-02,\n",
      "        2.9199e-03, 3.4091e-03, 5.3534e-03, 5.7586e-04, 1.3694e-03, 3.0247e-03,\n",
      "        2.0774e-03, 6.9300e-03, 5.0122e-03, 6.8698e-04, 6.0198e-04, 3.3802e-03,\n",
      "        3.6484e-03, 5.4962e-04, 3.4000e-03, 2.4014e-03, 2.5410e-03, 4.2855e-04,\n",
      "        1.6855e-03, 4.2442e-04, 4.3275e-04, 2.2966e-03, 3.8843e-07, 7.2256e-04,\n",
      "        6.5573e-04, 5.2527e-04, 4.0017e-04, 5.1596e-04, 1.0877e-03, 2.4073e-03,\n",
      "        2.1215e-04, 1.1172e-03, 9.8773e-04, 1.5049e-03, 8.0607e-04, 5.3372e-04,\n",
      "        4.1890e-04, 3.2356e-04, 8.1220e-04, 5.3407e-04, 9.4648e-05, 6.0742e-04,\n",
      "        1.6985e-04, 2.0873e-04, 1.3257e-04, 6.4601e-04, 4.8710e-04, 8.3591e-04,\n",
      "        5.0240e-04, 1.5002e-02, 1.0670e-04, 1.9948e-04, 1.9584e-01, 4.0140e-07,\n",
      "        1.0662e-01], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "sorted_prob_values: [0.19583642482757568, 0.19393040239810944, 0.14070133864879608, 0.10661710798740387, 0.07612410932779312, 0.07472309470176697, 0.044520847499370575, 0.04397302493453026, 0.015002288855612278, 0.011413884349167347, 0.009006449952721596, 0.006930026225745678, 0.005353420507162809, 0.005323335062712431, 0.0050122095271945, 0.004776509944349527, 0.0037473973352462053, 0.00364843406714499, 0.003597521223127842, 0.0034091377165168524, 0.0033999914303421974, 0.003380161477252841, 0.0030247331596910954, 0.0029199167620390654, 0.0025409983936697245, 0.0024073468521237373, 0.0024013963993638754, 0.0022965751122683287, 0.0020773564465343952, 0.00168546789791435, 0.0015049403300508857, 0.0013693677028641105, 0.0011171711375936866, 0.0010877455351874232, 0.0009877309203147888, 0.0008359134080819786, 0.000812202924862504, 0.0008060674881562591, 0.0007225594599731266, 0.0006869828794151545, 0.0006557321758009493, 0.0006460134172812104, 0.0006074185948818922, 0.0006019838619977236, 0.0005758574116043746, 0.0005496232770383358, 0.0005340692587196827, 0.0005337183829396963, 0.000525268551427871, 0.0005159631837159395, 0.0005024010897614062, 0.0004871019918937236, 0.0004327528877183795, 0.000428551051300019, 0.0004244229639880359, 0.00041889818385243416, 0.00040017470018938184, 0.0003235558106098324, 0.00021215360902715474, 0.00020873492758255452, 0.00019948356202803552, 0.0001698470878181979, 0.00013256787497084588, 0.00010669900802895427, 9.464757749810815e-05, 4.0139855173038086e-07, 3.884300667778007e-07]\n",
      "sorted_prob_indices: [64, 3, 2, 66, 4, 0, 5, 1, 61, 11, 8, 19, 14, 7, 20, 9, 6, 24, 10, 13, 26, 23, 17, 12, 28, 41, 27, 33, 18, 30, 45, 16, 43, 40, 44, 59, 50, 46, 35, 21, 36, 57, 53, 22, 15, 25, 51, 47, 37, 39, 60, 58, 32, 29, 31, 48, 38, 49, 42, 55, 63, 54, 56, 62, 52, 65, 34]\n",
      "result_smi_list: ['xxxx', 'C', 'O', 'EOS', 'CC', 'N', 'C=O', 'c1ccccc1', 'NS(=O)(=O)O', 'FC(F)F', 'C1CCNCC1', 'C1CCCCC1', 'C1CCNC1', 'Clc1ccccc1', 'c1cncnc1', 'Fc1ccccc1', 'c1ccncc1', 'c1c[nH]cn1', 'O=P(O)(O)O', 'NC=O', 'c1ccsc1', 'c1cn[nH]c1', 'C1CC1', 'CC(C)C', 'O=[SH](=O)c1ccccc1', 'C[SH](=O)=O', 'c1ccc2ccccc2c1', 'C1CCCC1', 'S', 'Clc1cccc(Cl)c1', 'C=CC', 'C1CNCCN1', 'N#Cc1ccccc1', 'O=c1cc[nH]c(=O)[nH]1', 'O=C1CCCN1', 'C1CCOCC1', 'c1ccc2ncncc2c1', 'c1ccoc1', 'Fc1cccc(F)c1', 'C1COCCN1', 'c1cscn1', 'c1ccc2[nH]ncc2c1', 'C1CCOC1', 'OC1COCC(O)C1O', 'OC1COCC1O', 'c1ccc2[nH]ccc2c1', 'N=C(N)N', 'N=C(N)c1ccccc1', 'c1ccc2[nH]cnc2c1', 'c1ccc2ncccc2c1', 'c1nc[nH]n1', 'CC1=C(C(=O)O)N2C(=O)CC2SC1', 'OC1CCOC1', 'Nc1ncnc2[nH]cnc12', 'O=[N+]([O-])c1ccccc1', 'c1ncc2nc[nH]c2n1', 'Oc1ccccc1O', 'C[NH+](C)C', 'Brc1ccccc1', 'c1ccc2c(c1)OCO2', 'c1ccc2c(c1)Nc1ccccc1S2', 'Clc1ccccc1Cl', 'CC1(C)CN2C(=O)CC2S1', 'OC1CCOCC1O', 'Nc1cc[nH]c(=O)n1', 'EOS', 'O=P(O[3*])(O)OP(=O)(O)O[3*]']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义模型、优化器、调度器等\n",
    "model = GPT(GPTConfig())\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)\n",
    "\n",
    "current_time = '2025-01-27__00-12-04'\n",
    "current_time = '2025-02-02__11-16-09'   \n",
    "epoch = '1000'\n",
    "epoch = '100'\n",
    "\n",
    "# 设定要加载的模型路径\n",
    "checkpoint_path = f'./model_save_pth/mini_QWEN_frag_recomd_{current_time}/model_epoch_{epoch}.pt'\n",
    "\n",
    "# 加载模型\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "\n",
    "# 加载状态字典\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "# 恢复训练时的epoch和val_loss\n",
    "epoch = checkpoint['epoch']\n",
    "val_loss = checkpoint['val_loss']\n",
    "\n",
    "# 如果你想恢复训练过程，可以继续从这里开始。\n",
    "x = torch.tensor([[1]]).to(device)\n",
    "y, probs = model.generate(x, 1)   # (B, T+1)   (B, vocab_size)\n",
    "print(f'y.shape: {y.shape}')\n",
    "print(f'y: {y}')\n",
    "y_list = y.squeeze(0).cpu().numpy().tolist()\n",
    "y_frag_list = [idx_to_fragment[y] for y in y_list]\n",
    "print(elem_frag_smi_list)\n",
    "print(f'y_frag_list: {y_frag_list}')\n",
    "print(f'probs.shape: {probs.shape}')\n",
    "y = y[:, -1].squeeze(0)\n",
    "probs = probs.squeeze(0)\n",
    "print(f'probs.shape: {probs.shape}')\n",
    "print(f'probs: {probs}')\n",
    "sorted_prob_values, sorted_prob_indices = torch.sort(probs, descending=True)\n",
    "sorted_prob_values = sorted_prob_values.tolist()\n",
    "sorted_prob_indices = sorted_prob_indices.tolist()\n",
    "print(f'sorted_prob_values: {sorted_prob_values}')\n",
    "print(f'sorted_prob_indices: {sorted_prob_indices}')\n",
    "result_smi_list = [idx_to_fragment[id] for id in sorted_prob_indices]\n",
    "print(f'result_smi_list: {result_smi_list}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
